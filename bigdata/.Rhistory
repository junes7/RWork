con$insert(final_mongo_data)
conn <- mongo(collection = "crawlpage",db = "bigdata",url = "mongodb://127.0.0.1")
if(conn$count()>0){
conn$drop()
}
class(final_data_list)
#mongodb에 데이터를 저장하기 위해서 dataframe으로 변환
final_mongo_data <- data.frame(final_data_list)
conn$insert(final_mongo_data)
library(stringr)
library(mongolite)
conn <- mongo(collection = "crawlpage",db = "bigdata",url = "mongodb://127.0.0.1")
# i <- 0
# class(i)
final_data_list=NULL
#urllink <- "https://www.clien.net/service/group/community?&od=T31&po="
for(i in c(0:9)){
urllinkpage <- paste0("https://www.clien.net/service/group/community?&od=T31&po=",i)
urllinkpage
url_data <- readLines(urllinkpage,encoding = "UTF-8")
filter_data <- url_data[str_detect(url_data,"subject_fixed")]
filter_data
title <- str_extract(filter_data,"(?<=\">).*(?=</span>)")
title <- gsub("\"","",title)
title
hit_data <- url_data[str_detect(url_data,"<span class=\"hit\">")]
hit_data
hit <- str_extract(hit_data,"(?<=\">).*(?=</span>)")[-1]
hit
str_detect(url_data,"subject_fixed")
#TRUE인 위치들의 값을 뽑아준다.
str_detect(url_data,"subject_fixed")
which(str_detect(url_data,"subject_fixed"))
(which(str_detect(url_data,"subject_fixed"))-3)
myurl <- url_data[(which(str_detect(url_data,"subject_fixed"))-3)]
url_val <- str_extract(myurl,"(?<=href=\").*(?=data-role)")
url_val
#필요없는 문자열을 잘라내기 end = -3 : 뒤에서 3개를 잘라내기
url_val <- str_sub(url_val, end = -3)
url_val
url <- paste0("https://www.clien.net",url_val)
url
####url을 이용해서 content 항목 추출####
contentlist=NULL #최초 변수 선언 시 null로 초기화
content_data=NULL
for(page in 1:length(url)){
content <- readLines(url[page],encoding = "UTF-8")
start = which(str_detect(content,"<article>"))
end = which(str_detect(content,"</article>"))
content_filter <- content[start:end]
content_data <- paste(content_filter,collapse = "")
content_data <- gsub("<.*?>","",content_data)
content_data <- gsub("\t|&nbsp;|\"| ","",content_data)
#기존의 저장되어 있는 contentlist의 내용에 추가
contentlist <- c(contentlist,content_data)
cat("\n",page)
}
final_data <- cbind(title,hit,url,contentlist)
final_data_list <- rbind(final_data_list,final_data)
#cat("\n","outer:"+i)
}
write.csv(final_data_list,"crawl_data_list.csv")
#위의 .csv파일로 저장해 읽어들이는 것보다 밑의 .RData로 저장해 읽어들이는 것이 훨씬 빠르다.
save(final_data_list,file = "crawl_data_list.RData")
####mongodb에 저장하는 작업####
if(conn$count()>0){
conn$drop()
}
class(final_data_list)
#mongodb에 데이터를 저장하기 위해서 dataframe으로 변환
final_mongo_data <- data.frame(final_data_list)
conn$insert(final_mongo_data)
install.packages("N2H4")
library(N2H4)
comments <- getAllComment(url)
url <- "https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=100&oid=020&aid=0003276790"
comments <- getAllComment(url)
comments
getAllComment(url) %>%
select(username,contents) -> comments
comments
getAllComment(url) %>%
select(userName,contents) -> comments
comments
comments <- getAllComment(url)
comments
comments %>%
select(userName,contents) -> comments
comments
url <- "https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=100&oid=020&aid=0003276790"
comments <- getAllComment(url)
comments
comments %>%
select(userName,contents) -> mydata
mydata
library(dplyr)
url <- "https://news.naver.com/main/read.nhn?mode=LSD&mid=shm&sid1=100&oid=020&aid=0003276790"
comments <- getAllComment(url)
comments
comments %>%
select(userName,contents) -> mydata
mydata
class(myda)
class(mydata)
mycomment <- mydata$contents
mycomment
#css의 선택자를 활용할 수 있는 방법 - rvest
install.packages("rvest")
install.packages("rvest")
library(rvest)
#위의 rvest가 내부적으로 xml2를 사용하고 있다.
readPage <- read_html(url)
readPage
#css의 선택자를 활용할 수 있는 방법 - rvest
install.packages("rvest")
library(rvest)
url <- "https://www.clien.net/service/group/community?&od=T31&po=0"
#위의 rvest가 내부적으로 xml2를 사용하고 있다.
readPage <- read_html(url)
library(rvest)
url <- "https://www.clien.net/service/group/community?&od=T31&po=0"
#위의 rvest가 내부적으로 xml2를 사용하고 있다.
readPage <- read_html(url)
readPage %>%
html_nodes("span.subject_fixed") -> title_data
readPage
url <- "https://www.clien.net/service/group/community?&od=T31&po=0"
#위의 rvest가 내부적으로 xml2를 사용하고 있다.
readPage <- read_html(url)
readPage %>%
html_nodes("span.subject_fixed") %>%
html_text() -> title_data
title_data
library(rvest)
url <- "https://www.clien.net/service/group/community?&od=T31&po=0"
#위의 rvest가 내부적으로 xml2를 사용하고 있다.
readPage <- read_html(url)
# html_nodes(선택자): 일치하는 모든 태그를 추출
# html_node(선택자): 일치하는 태그 한 개
# html_text(): 텍스트를 추출
# html_name(): 하위 노드 추출
# html_attr():  attribute 추출
readPage %>%
html_nodes("span.subject_fixed") %>%
html_text() -> title_data
title_data
library(mongolite)
library(rvest)
library(stringr)
library(dplyr)
library(mongolite)
install.packages("KoNLP")
#지금 R 3.6.3버전에서는 KoNLP가 지원이 안 된다.
library(KoNLP)
install.packages("sejong")
install.packages("KoNLP")
#지금 R 3.6.3버전에서는 KoNLP가 지원이 안 된다.
library(KoNLP)
install.packages("hash")
install.packages("rJava")
install.packages("tau")
install.packages("RSQLite")
install.packages("devtools")
install.packages("hash")
install.packages("sejong")
install.packages("KoNLP")
install.packages("sejong")
install.packages("sejong")
source('C:/iot/work/RWork/bigdata/konlp.R', encoding = 'UTF-8', echo=TRUE)
install.packages("KoNLP")
#지금 R 3.6.3버전에서는 KoNLP가 지원이 안 된다.
library(KoNLP)
install.packages("sejong")
install.packages("rJava")
install.packages("tau")
install.packages("RSQLite")
install.packages("devtools")
install.packages("sejong")
#지금 R 3.6.3버전에서는 KoNLP가 지원이 안 된다.
library(KoNLP)
install.packages("stringr")
install.packages("dplyr")
#지금 R 3.6.3버전에서는 KoNLP가 지원이 안 된다.
library(KoNLP)
install.packages("dplyr")
#지금 R 3.6.3버전에서는 KoNLP가 지원이 안 된다.
library(KoNLP)
install.packages("sejong")
#지금 R 3.6.3버전에서는 KoNLP가 지원이 안 된다.
library(KoNLP)
install.packages("Sejong")
install.packages("hash")
#지금 R 3.6.3버전에서는 KoNLP가 지원이 안 된다.
library(KoNLP)
install.packages("rJava")
#지금 R 3.6.3버전에서는 KoNLP가 지원이 안 된다.
library(KoNLP)
install.packages("tau")
#지금 R 3.6.3버전에서는 KoNLP가 지원이 안 된다.
library(KoNLP)
install.packages("RSQLite")
install.packages("devtools")
#지금 R 3.6.3버전에서는 KoNLP가 지원이 안 된다.
library(KoNLP)
#지금 R 3.6.3버전에서는 KoNLP가 지원이 안 된다.
library(KoNLP)
install.packages("RSQLite")
install.packages("mongolite")
install.packages("RSQLite")
install.packages("devtools")
#지금 R 3.6.3버전에서는 KoNLP가 지원이 안 된다.
library(KoNLP)
library(Sejong)
library(hash)
library(rJava)
library(tau)
library(RSQLite)
library(devtools)
useSejongDic()
####분석할 샘플데이터 로딩####
load("comments.RData")
length(comments)
length(score)
head(comments,10)
head(comments,10)
head(score,10)
sampledata <- comments[1:1000]
class(sampledata)
####분석할 샘플데이터 로딩####
load("comments.RData")
load("score.RData")
length(comments)
length(score)
head(comments,10)
head(score,10)
sampledata <- comments[1:1000]
class(sampledata)
extractNoun("롯데마트가 판매하고 있는 흑마늘 양념 치킨이 논란이 되고 있다.")
extractNoun(c("R은 free 소프트웨어이고, [완전하게 무보증]입니다.",
"일정한 조건에 따르면, 자유롭게 이것을 재배포할수가 있습니다.")
)
pos <- SimplePos09("롯데마트가 판매하고 있는 흑마늘 양념 치킨이 논란이 되고 있다.")
pos
SimplePos22("롯데마트가 판매하고 있는 흑마늘 양념 치킨이 논란이 되고 있다.")
####형태소 분석을 하기 위해서 명사분리####
#댓글을 분리하면 분리된 명사의 개수가 다르므로 사이즈에 상관없는 리스트를 이용하여야 한다.
data_list = list()
sampledata <- comments[1:1000]
class(sampledata)
data_list = list()
for(i in 1:length(sampledata)){
data <- SimplePos09(sampledata[i])
data_list[[i]] <- data
}
data_list[[2]]
data_list[[22]]
head(data_list,20)
sampledata <- comments[1:1000]
head(comments,10)
sapply(data.frame(test=c(1,2,3,4,5,6),
test2=c(3,4,5,6,7,8)
), #반복작업하 데이터
function(x){
x[1] #반복해서 적용할 함수
})
wordlist <- sapply(strsplit(data_data_list,"/"), function(x){
x[1]
})
wordlist
wordlist <- sapply(strsplit(data_list,"/"), function(x){
x[1]
})
wordlist
wordlist <- sapply(str_split(data_list,"/"), function(x){
x[1]
})
wordlist
data_list = list()
for(i in 1:length(sampledata)){
data <- SimplePos09(sampledata[i])
data_list[[i]] <- data
}
data_list[[22]]
head(data_list,20)
wordlist <- sapply(strsplit(data_list,"/"), function(x){
x[1]
})
wordlist
wordlist <- sapply(str_split(data_list,"/"), function(x){
x[1]
})
wordlist
#단어 하나에는 명사가 하나 나온다.
#/로 분할 - 리스트의 모든 요소에 저정된 문자열을 /로 분리
#   => N이 있는 문자열의 첫 번째 요소 가져오기
#sapply를 이용하면 반복작업을 할 수 있다.
# sapply(data.frame(test=c(1,2,3,4,5,6),
#                   test2=c(3,4,5,6,7,8)
#                   ), #반복작업하 데이터
#        function(x){
#          x[1] #반복해서 적용할 함수
#        })
class(data_list)
#단어 하나에는 명사가 하나 나온다.
#/로 분할 - 리스트의 모든 요소에 저정된 문자열을 /로 분리
#   => N이 있는 문자열의 첫 번째 요소 가져오기
#sapply를 이용하면 반복작업을 할 수 있다.
# sapply(data.frame(test=c(1,2,3,4,5,6),
#                   test2=c(3,4,5,6,7,8)
#                   ), #반복작업하 데이터
#        function(x){
#          x[1] #반복해서 적용할 함수
#        })
wordlist = NULL
class(data_list)
wordlist <- sapply(str_split(data_list,"/"), function(x){
x[1]
})
wordlist
library(stringr)
install.packages("stringr")
install.packages("dplyr")
library(dplyr)
library(mongolite)
wordlist <- sapply(str_split(data_list,"/"), function(x){
x[1]
})
wordlist
tail(comments,10)
head(comments,1000)
wordlist
load("comments.RData")
load("score.RData")
length(comments)
length(score)
head(comments,10)
head(score,10)
head(comments,1000)
sampledata <- comments[1:1000]
class(sampledata)
data_list = list()
for(i in 1:length(sampledata)){
data <- SimplePos09(sampledata[i])
data_list[[i]] <- data
}
data_list[[22]]
head(data_list,20)
library(KoNLP)
library(stringr)
library(dplyr)
library(Sejong)
library(hash)
library(rJava)
library(tau)
library(mongolite)
library(RSQLite)
library(devtools)
data_list = list()
for(i in 1:length(sampledata)){
data <- SimplePos09(sampledata[i])
data_list[[i]] <- data
}
data_list[[22]]
head(data_list,20)
convertHangulStringToJamos("R는 많은 공헌자에의한 공동 프로젝트입니다")
wordlist = NULL
class(data_list)
wordlist <- sapply(str_split(data_list,"/"), function(x){
x[1]
})
wordlist
data_list = list()
for(i in 1:length(sampledata)){
data <- SimplePos09(sampledata[i])
data_list[[i]] <- data
}
data_list[[22]]
head(data_list,20)
data_list[[22]]
data_list[[22]]
head(comments,10)
class(wordlist)
head(wordlist,20)
head(data_list,20)
class(data_list)
#리스트를 unlist로 변환
class(unlist(data_list))
wordlist <- sapply(str_split(unlist(data_list),"/"),
function(x){
x[1]
})
wordlist
class(wordlist)
head(wordlist,20)
head(data_list,20)
head(wordlist,20)
unlis
wordlist <- sapply(str_split(unlist,"/"),
function(x){
x[1]
})
wordlist
head(wordlist,20)
#table함수를 이용해서 단어의 빈도수를 구하기
tablewordlist <- table(wordlist)
tablewordlist[1]
tablewordlist[89]
names(tablewordlist)
#분석한 데이터를 이용해서 sort
sort(tablewordlist,decreasing = T)[1:100]
#분석결과를 가지고 기준을 적용해서 정리 - 한 글자를 빼고 작업
nchar("글자수")
tablewordlist_result <- tablewordlist[nchar(names(tablewordlist))>1]
#wordlist안에는 형태소를 분리해낸 단어들이 저장되어 나온다.
class(wordlist)
head(wordlist,20)
head(data_list,20)
#table함수를 이용해서 단어의 빈도수를 구하기
#table함수는 벡터에 저장되어 있는 모든 단어들의
#빈도수를 계산해서 변환 - 단어를 이용해서
#변수명으로 사용
tablewordlist <- table(wordlist)
tablewordlist[1]
tablewordlist[89]
names(tablewordlist)
#분석한 데이터를 이용해서 sort
sort(tablewordlist,decreasing = T)[1:100]
#분석결과를 가지고 기준을 적용해서 정리 - 한 글자를 빼고 작업
nchar("글자수")
tablewordlist_result <- tablewordlist[nchar(names(tablewordlist))>1]
tablewordlist_result <- sort(tablewordlist_result,decreasing = T)[1:100]
tablewordlist_result
tablewordlist_result
install.packages("wordcloud")
install.packages("RColorBrewer")
library(wordcloud)
library(RColorBrewer)
#RColorBrewer
display.brewer.all(n = 10,exact.n = FALSE)
brewer.pal.info
display.brewer.all(type="div")
display.brewer.all(type="qual")
display.brewer.all(type="seq")
display.brewer.all(type="seq")
display.brewer.all(type="qual")
display.brewer.all(type="div")
#분석한 결과가 저장되어 있는 tablewordlist_result의
#값을 단어와 숫자를 각각 저장
word <- names(tablewordlist_result)
count <- as.numeric(tablewordlist_result)
display.brewer.all(type="seq")
display.brewer.all(type="qual")
brewer.pal.info
mycolor <- brewer.pal(n = 11,name = "RdYlGn")
word <- names(tablewordlist_result)
count <- as.numeric(tablewordlist_result)
mycolor <- brewer.pal(n = 11,name = "RdYlGn")
wordcloud(words = word,freq = count,
random.order = F, colors = mycolor,
scale = c(7,0,3))
word <- names(tablewordlist_result)
count <- as.numeric(tablewordlist_result)
#mycolor <- brewer.pal(n = 11,name = "RdYlGn")
mycolor <- brewer.pal(n = 9,name = "set1")
wordcloud(words = word,freq = count,
random.order = F, colors = mycolor,
scale = c(7,0,3))
word <- names(tablewordlist_result)
count <- as.numeric(tablewordlist_result)
#mycolor <- brewer.pal(n = 11,name = "RdYlGn")
mycolor <- brewer.pal(n = 9,name = "set1")
wordcloud(words = word,freq = count,
random.order = F, colors = mycolor,
scale = c(7,0,3))
word <- names(tablewordlist_result)
count <- as.numeric(tablewordlist_result)
#mycolor <- brewer.pal(n = 11,name = "RdYlGn")
mycolor <- brewer.pal(n = 9,name = "set1")
wordcloud(words = word,freq = count,
random.order = F, colors = mycolor,
scale = c(7,0,3))
brewer.pal.info
#RColorBrewer
display.brewer.all(n = 10,exact.n = FALSE)
wordcloud(words = word,freq = count,
random.order = F, colors = mycolor,
scale = c(7,0,3))
tablewordlist_result <- tablewordlist[nchar(names(tablewordlist))>1]
tablewordlist_result <- sort(tablewordlist_result,decreasing = T)[1:100]
tablewordlist_result
word <- names(tablewordlist_result)
count <- as.numeric(tablewordlist_result)
#mycolor <- brewer.pal(n = 11,name = "RdYlGn")
mycolor <- brewer.pal(n = 9,name = "set1")
wordcloud(words = word,freq = count,
random.order = F, colors = mycolor,
scale = c(7,0,3))
word <- names(tablewordlist_result)
count <- as.numeric(tablewordlist_result)
#mycolor <- brewer.pal(n = 11,name = "RdYlGn")
mycolor <- brewer.pal(n = 12,name = "set3")
wordcloud(words = word,freq = count,
random.order = F, colors = mycolor,
scale = c(7,0,3))
#mycolor <- brewer.pal(n = 11,name = "RdYlGn")
mycolor <- brewer.pal(n = 8,name = "set2")
wordcloud(words = word,freq = count,
random.order = F, colors = mycolor,
scale = c(7,0,3))
wordcloud(words = word,freq = count,
random.order = F, colors = mycolor,
scale = c(7,0,3))
